La **regularizaci칩n** es una de las t칠cnicas m치s potentes para rescatar a un modelo de las garras del **sobreajuste (overfitting)**. Si la regresi칩n lineal est치ndar es un estudiante que intenta memorizar cada punto de los datos, la regularizaci칩n es el tutor que le dice: "C칠ntrate en lo importante y no te compliques demasiado con los detalles irrelevantes".

---

## Regresi칩n Regularizada: Ridge y Lasso

### 쯇or qu칠 regularizar?

En una regresi칩n lineal est치ndar (OLS), el modelo intenta minimizar la funci칩n de p칠rdida eligiendo coeficientes ($a$) para cada caracter칤stica. Si permitimos que estos coeficientes crezcan sin control, el modelo se vuelve demasiado sensible al ruido de los datos de entrenamiento, lo que causa sobreajuste.

La regularizaci칩n **penaliza los coeficientes grandes** modificando la funci칩n de p칠rdida. Es como ponerle un "impuesto" a la complejidad del modelo.

---

### Regresi칩n Ridge

La Regresi칩n Ridge a침ade a la funci칩n de p칠rdida el cuadrado de los coeficientes multiplicado por una constante llamada **Alpha ($\alpha$)**.

- **Alpha ($\alpha$):** Es un hiperpar치metro que controla la complejidad.
    
    - **$\alpha = 0$:** Es una regresi칩n lineal normal (peligro de sobreajuste).
        
    - **$\alpha$ muy alto:** Penaliza tanto los coeficientes que pueden terminar siendo casi cero, provocando **ajuste insuficiente (underfitting)**.
        

> 游눠 **Analog칤a del equipaje:** Imagina que vas de viaje (entrenar el modelo). OLS es llevar todas las maletas que quieras. Ridge es una aerol칤nea que te cobra por el **cuadrado del peso** de cada maleta. Intentar치s llevar solo lo necesario y que nada sea excesivamente pesado para no pagar una fortuna.


```python
from sklearn.linear_model import Ridge

# Crear lista para guardar puntuaciones
scores = []

# Probar diferentes alphas
for alpha in [0.1, 1.0, 10.0, 100.0, 1000.0]:
    ridge = Ridge(alpha=alpha)
    ridge.fit(X_train, y_train)
    
    # Evaluar el rendimiento (R-cuadrado)
    score = ridge.score(X_test, y_test)
    scores.append(score)

print(scores)
```

---

### Regresi칩n Lasso

Lasso (_Least Absolute Shrinkage and Selection Operator_) funciona de forma similar a Ridge, pero en lugar de usar el cuadrado de los coeficientes, utiliza su **valor absoluto**.

- **Diferencia clave:** Mientras que Ridge reduce los coeficientes pero rara vez los hace cero, Lasso tiene la capacidad de **reducir coeficientes a cero exactamente**.
    

#### Lasso para la Selecci칩n de Caracter칤sticas

Debido a que Lasso puede eliminar caracter칤sticas (poniendo su coeficiente en 0), es una herramienta incre칤ble para identificar qu칠 variables son realmente importantes y cu치les son solo "ruido".

#### C칩digo y Visualizaci칩n de Importancia

Python

```
from sklearn.linear_model import Lasso
import matplotlib.pyplot as plt

# Obtener los nombres de las columnas
names = sales_df.drop("sales", axis=1).columns

# Instanciar Lasso con un alpha peque침o
lasso = Lasso(alpha=0.1)

# Ajustar a TODOS los datos para ver importancia
lasso.fit(X, y)

# Extraer los coeficientes
lasso_coef = lasso.coef_

# Graficar la importancia de cada caracter칤stica
plt.plot(range(len(names)), lasso_coef)
plt.xticks(range(len(names)), names, rotation=60)
plt.ylabel("Coeficientes")
plt.show()
```

---

### Explicaci칩n del Gr치fico de Lasso

Al ejecutar el c칩digo anterior, ver치s un gr치fico con picos y valles:

- **Picos altos (positivos o negativos):** Representan las caracter칤sticas que m치s influyen en el resultado (ej. en el caso de la glucosa, el factor "diabetes" tendr치 un coeficiente muy alto).
    
- **L칤nea en cero:** Son las caracter칤sticas que Lasso "descart칩". El modelo decidi칩 que no aportan informaci칩n 칰til para la predicci칩n.
    

**Beneficios de Lasso:**

1. **Claridad:** Permite explicar resultados a audiencias no t칠cnicas ("Estas 3 variables son las que mueven el negocio").
    
2. **Eficiencia:** Reduce la cantidad de datos necesarios para futuras predicciones al ignorar lo irrelevante.
    

쯊e gustar칤a que hagamos un ejercicio pr치ctico para comparar los coeficientes de una Regresi칩n Lineal normal frente a una de Lasso y ver cu치ntas variables "mueren" en el proceso?