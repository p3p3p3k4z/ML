En este ejercicio final de regresi칩n, compararemos tres algoritmos fundamentales: **Regresi칩n Lineal**, **Ridge** y **Lasso**. El objetivo es predecir los niveles de "energy" en las canciones del dataset `music_df`. Al utilizar un bucle para evaluar m칰ltiples modelos simult치neamente, podemos identificar no solo cu치l es m치s preciso en promedio, sino cu치l es m치s consistente a trav칠s de diferentes subconjuntos de datos gracias al diagrama de cajas.

```python
import matplotlib.pyplot as plt

# Diccionario de modelos para comparar
models = {"Linear Regression": LinearRegression(), 
          "Ridge": Ridge(alpha=0.1), 
          "Lasso": Lasso(alpha=0.1)}
results = []

# 1. Bucle para iterar sobre los modelos
for model in models.values():
  # Configurar la validaci칩n cruzada con el random_state correcto (42)
  kf = KFold(n_splits=6, random_state=42, shuffle=True)
  
  # 2. Realizar la validaci칩n cruzada
  cv_results = cross_val_score(model, X_train, y_train, cv=kf)
  
  # 3. A침adir los resultados a la lista
  results.append(cv_results)

# 4. Crear el diagrama de cajas (boxplot)
plt.boxplot(results, labels=models.keys())
plt.ylabel("R-squared")
plt.show()
```

---

### An치lisis de la Comparativa de Regresi칩n

Esta t칠cnica es la forma m치s profesional de tomar una decisi칩n basada en datos sobre qu칠 algoritmo desplegar en producci칩n.

#### 1. 쯇or qu칠 estos tres modelos?

- **Linear Regression:** Es nuestra l칤nea base. No tiene regularizaci칩n y es el m치s simple.
    
- **Ridge:** A침ade una penalizaci칩n $L_2$ (al cuadrado de los coeficientes) para evitar que el modelo se vuelva demasiado complejo.
    
- **Lasso:** A침ade una penalizaci칩n $L_1$ (valor absoluto), lo que puede reducir algunos coeficientes a cero, realizando una selecci칩n autom치tica de caracter칤sticas.
    

#### 2. Interpretaci칩n del Boxplot

Al visualizar los resultados, no solo mires la l칤nea central (la mediana). F칤jate en el tama침o de la caja:

- **Caja compacta:** Indica que el modelo es **robusto**. Independientemente de c칩mo dividas los datos, el error es similar.
    
- **Caja alargada o con valores at칤picos (outliers):** Indica que el modelo es sensible a los datos de entrenamiento y podr칤a sufrir de varianza alta.
    

#### 3. El R-cuadrado ($R^2$) como m칠trica

Recuerda que en este ejercicio estamos usando el coeficiente de determinaci칩n. Un valor m치s cercano a **1.0** indica que el modelo explica mejor la variabilidad de la "energ칤a" de la canci칩n. Si ves que Lasso tiene una puntuaci칩n mucho menor, es posible que el valor de `alpha=0.1` sea demasiado alto y est칠 eliminando caracter칤sticas que s칤 eran importantes.

---

### Nota de Ingenier칤a (Enfoque DevOps)

Como aspirante a **SysAdmin/DevOps**, este patr칩n de c칩digo es altamente automatizable. Puedes integrar este script en un contenedor de Docker para que, cada vez que el equipo de ciencia de datos actualice el dataset, el sistema genere autom치ticamente este gr치fico y lo env칤e a un dashboard (como Grafana o un reporte de MLflow). Esto permite detectar de inmediato si un nuevo conjunto de datos ha hecho que el modelo previamente elegido deje de ser el 칩ptimo.

---
Este es el momento de la verdad. Despu칠s de comparar modelos mediante validaci칩n cruzada, el paso final es enfrentarlos a los datos de prueba (**test set**). Esta es la evaluaci칩n definitiva porque utiliza datos que los modelos jam치s han visto durante su entrenamiento. Usaremos el **RMSE** (_Root Mean Squared Error_), que nos da una medida del error en las mismas unidades que nuestro objetivo ("energy"), facilitando mucho la interpretaci칩n t칠cnica.

```python
# Importar root_mean_squared_error de sklearn.metrics
from sklearn.metrics import root_mean_squared_error

# Iterar sobre el diccionario de modelos
for name, model in models.items():
  # Ajustar el modelo a los datos de entrenamiento escalados
  model.fit(X_train_scaled, y_train)
  
  # Hacer predicciones sobre el conjunto de pruebas escalado
  y_pred = model.predict(X_test_scaled)
  
  # Calcular el RMSE del conjunto de pruebas
  # Pasamos las etiquetas reales y las predicciones
  test_rmse = root_mean_squared_error(y_test, y_pred)
  print("{} Test Set RMSE: {}".format(name, test_rmse))
```

---

## An치lisis Final: El Duelo de Regresores

En este ejercicio, estamos comparando la **Regresi칩n Lineal** frente a **Ridge**. Como ingeniero, aqu칤 tienes los puntos clave para interpretar estos resultados:

### 쯈u칠 nos dice el RMSE?

El RMSE cuantifica qu칠 tan lejos, en promedio, est치n las predicciones del modelo de los valores reales. Al ser una ra칤z cuadrada, penaliza m치s fuertemente los errores grandes que el MAE (_Mean Absolute Error_).

- **Si el RMSE es bajo:** El modelo es preciso.
    
- **Si el RMSE de prueba es mucho mayor que el de entrenamiento:** Tienes un problema de **sobreajuste (overfitting)**.
    

### Regresi칩n Lineal vs. Ridge

- **Linear Regression:** Intenta ajustar la l칤nea que minimiza la suma de los errores al cuadrado sin restricciones. Si los datos tienen ruido, puede volverse demasiado compleja.
    
- **Ridge:** Introduce una penalizaci칩n para mantener los coeficientes bajo control. Si el RMSE de Ridge es menor que el de la Regresi칩n Lineal, significa que la regularizaci칩n ayud칩 a que el modelo generalizara mejor.
    

---

> 游눠 **Tip de Despliegue (DevOps/SysAdmin):** > En un flujo de trabajo profesional, el modelo que obtenga el RMSE m치s bajo en el **test set** es el que "empaquetar칤as" (usando librer칤as como `joblib` o `pickle`) para subirlo a un servidor de producci칩n. Recuerda siempre guardar tambi칠n el objeto `scaler`, ya que las predicciones en vivo necesitar치n ser escaladas exactamente con los mismos par치metros de media y varianza que usaste aqu칤.
