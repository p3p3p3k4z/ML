En este primer paso para limpiar el conjunto de datos `music_df`, realizaremos un diagn√≥stico inicial. Antes de construir cualquier modelo o aplicar t√©cnicas de imputaci√≥n, es fundamental cuantificar cu√°ntos "huecos" (valores `NaN`) tiene cada caracter√≠stica. Esto nos permitir√° decidir, bas√°ndonos en la regla del 5%, qu√© columnas podemos limpiar por eliminaci√≥n y cu√°les requerir√°n una estrategia m√°s avanzada.

```python
# Imprimir los valores faltantes por cada columna
# .isna() identifica los nulos, .sum() los cuenta y .sort_values() los ordena ascendentemente
print(music_df.isna().sum().sort_values())
```

---

### An√°lisis del Diagn√≥stico de Datos

Como ingeniero, este es tu primer punto de control en cualquier tuber√≠a de datos (_data pipeline_). Aqu√≠ te explico por qu√© este comando es tan potente:

- **Identificaci√≥n de Patrones:** Al ordenar de forma ascendente, ves inmediatamente qu√© columnas est√°n "limpias" y cu√°les podr√≠an ser problem√°ticas. Si una columna tiene much√≠simos valores faltantes, eliminar las filas podr√≠a sesgar el modelo o dejarnos sin datos suficientes para entrenar.
    
- **La Regla del 5%:** Si el conjunto de datos tiene 1,000 filas (como en este caso), cualquier columna con menos de 50 valores faltantes se considera candidata para la eliminaci√≥n directa (`.dropna()`). Esto simplifica el preprocesamiento sin sacrificar la integridad estad√≠stica del dataset.
    
- **Preparaci√≥n para la Imputaci√≥n:** Las columnas que superen ese 5% de valores nulos no se eliminar√°n; en el siguiente paso, definiremos c√≥mo rellenar esos huecos (usando la media o la moda) para que el modelo no pierda esa valiosa informaci√≥n.

---
Una vez identificado qu√© columnas tienen una cantidad m√≠nima de datos faltantes, aplicamos la t√©cnica de **eliminaci√≥n selectiva**. En este caso, como el dataset original tiene 1,000 filas, el 5% equivale a 50 registros. Eliminar estas filas es una forma eficiente de limpiar el ruido inicial sin comprometer la estructura de nuestro modelo KNN.

```python
# 1. Visualizar el conteo de nulos (Paso anterior para referencia)
print(music_df.isna().sum().sort_values())

# 2. Eliminar filas con nulos en las columnas que cumplen la regla del < 5%
# Especificamos las columnas en el argumento 'subset'
music_df = music_df.dropna(subset=["genre", "popularity", "loudness", "liveness", "tempo"])
```

---

### An√°lisis de la Eliminaci√≥n Selectiva

Como aspirante a **SysAdmin y DevOps**, sabes que la eficiencia en el procesamiento es clave. Aqu√≠ te explico por qu√© usamos `subset` en lugar de borrar todo:

- **Precisi√≥n con `subset`**: Si us√°ramos simplemente `music_df.dropna()`, Pandas eliminar√≠a cualquier fila que tuviera **al menos un** valor nulo en **cualquier** columna. Esto podr√≠a hacernos perder much√≠sima informaci√≥n de columnas que s√≠ planeamos imputar despu√©s (como las que tienen m√°s de 50 nulos).
    
- **La Regla del 5% en Producci√≥n**: Eliminar datos es una decisi√≥n dr√°stica. En ingenier√≠a de datos, se considera que perder menos del 5% de las muestras es un "costo aceptable" a cambio de no introducir ruido artificial (imputaciones) en variables que ya son lo suficientemente densas.
    
- **Preparaci√≥n de la Tuber√≠a**: Al limpiar estas columnas primero, garantizamos que las etiquetas cr√≠ticas (como `genre`) y las caracter√≠sticas con alta integridad est√©n listas. Esto facilita que el siguiente paso de **imputaci√≥n** se concentre √∫nicamente en las columnas que realmente tienen un problema de datos masivo.
    

---

Para cerrar este primer paso de limpieza, transformaremos nuestro problema en uno de **clasificaci√≥n binaria**. Muchos algoritmos funcionan mejor (o exclusivamente) con objetivos num√©ricos. Al convertir la columna "genre" en valores de 1 y 0, estamos preparando el terreno para que el modelo KNN pueda identificar qu√© patrones separan al "Rock" de cualquier otro estilo musical.

```python
# 1. Visualizar nulos iniciales (Paso 1)
print(music_df.isna().sum().sort_values())

# 2. Eliminar filas con nulos en columnas con < 5% de nulos (Paso 2)
music_df = music_df.dropna(subset=["genre", "popularity", "loudness", "liveness", "tempo"])

# 3. Convertir la columna 'genre' en una caracter√≠stica binaria
# Si es "Rock" asignamos 1, de lo contrario 0
music_df["genre"] = np.where(music_df["genre"] == "Rock", 1, 0)

# Verificaci√≥n final de nulos y dimensiones
print(music_df.isna().sum().sort_values())
print("Shape of the `music_df`: {}".format(music_df.shape))
```

---

### An√°lisis de la Transformaci√≥n Binaria con `np.where`

Como futuro **SysAdmin/DevOps**, apreciar√°s que la eficiencia de `numpy` es significativamente mayor que usar bucles `for` tradicionales. Aqu√≠ te explico la l√≥gica:

- **La potencia de `np.where`**: Funciona de manera vectorizada, similar a un "IF" en Excel o un operador ternario en C. Eval√∫a una condici√≥n en toda la columna a la vez, lo que lo hace extremadamente r√°pido incluso en datasets de gran tama√±o.
    
    - `np.where(condici√≥n, valor_si_se_cumple, valor_si_no)`
        
- **Simplificaci√≥n del Objetivo**: Al pasar de m√∫ltiples g√©neros a simplemente "Rock vs. Otros", estamos reduciendo la complejidad del problema. Esto es muy com√∫n cuando el negocio solo est√° interesado en detectar un evento espec√≠fico (ej. "Intrusi√≥n" vs. "Normal" en logs de red).
    
- **Estado Final del Dataset**: Tras estos tres pasos, has eliminado el ruido insignificante y has estandarizado la etiqueta objetivo. Notar√°s que la forma (_shape_) del DataFrame se ha reducido ligeramente debido al `dropna`, pero la calidad de los datos restantes es ahora mucho mayor para el entrenamiento.
    

> üí° **Tip de Ingenier√≠a**: En flujos de trabajo de **MLOps**, este tipo de transformaciones suelen ser el primer paso de un script de "Data Cleaning". Asegurarte de que el objetivo sea binario facilita mucho el c√°lculo de m√©tricas posteriores como la precisi√≥n, el recall y la curva ROC que vimos anteriormente.
