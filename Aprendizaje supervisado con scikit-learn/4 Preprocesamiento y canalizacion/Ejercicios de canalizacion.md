En este paso, daremos el salto hacia la automatizaci贸n profesional mediante el uso de **Pipelines**. Una canalizaci贸n nos permite agrupar la limpieza de datos (imputaci贸n) y el entrenamiento del modelo (KNN) en un solo objeto. Esto es fundamental para evitar la fuga de datos (_data leakage_) y asegurar que cualquier dato nuevo que entre al sistema reciba exactamente el mismo tratamiento que los datos de entrenamiento.

```python
# Importar los m贸dulos necesarios
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline

# 1. Instanciar un imputador
# Por defecto, SimpleImputer utiliza la media para rellenar valores num茅ricos
imputer = SimpleImputer()

# 2. Instanciar un modelo KNN con tres vecinos
knn = KNeighborsClassifier(n_neighbors=3)

# 3. Construir los pasos para la canalizaci贸n
# Cada paso es una tupla: ("nombre_del_paso", objeto_instanciado)
steps = [("imputer", imputer), 
         ("knn", knn)]
```

---

### An谩lisis del Pipeline y la Automatizaci贸n

Como aspirante a **SysAdmin/DevOps**, puedes ver un Pipeline como un _script_ de automatizaci贸n o un flujo de CI/CD para tus datos. Aqu铆 te explico por qu茅 esta estructura es la preferida en entornos de producci贸n:

- **Encapsulamiento:** En lugar de gestionar el imputador y el modelo por separado, el Pipeline los trata como una sola unidad. Al llamar a `pipeline.fit()`, scikit-learn ejecuta autom谩ticamente el `fit_transform` del imputador y luego el `fit` del modelo.
    
- **Prevenci贸n de Data Leakage:** El Pipeline garantiza que las estad铆sticas utilizadas para la imputaci贸n (como la media) se calculen 煤nicamente con los datos de entrenamiento y luego se apliquen al conjunto de prueba, manteniendo la integridad de la evaluaci贸n.
    
- **Intercambiabilidad:** Si ma帽ana decides cambiar el `SimpleImputer` por uno que use la mediana, o el KNN por una Regresi贸n Log铆stica, solo tienes que modificar un paso en la lista `steps`. El resto de tu c贸digo de entrenamiento y evaluaci贸n permanecer谩 intacto.
    

>  **Nota T茅cnica:** En un Pipeline, todos los pasos intermedios **deben ser transformadores** (objetos que tengan los m茅todos `.fit()` y `.transform()`). El 煤ltimo paso debe ser un **estimador** (el modelo que hace la predicci贸n).

---
En esta segunda parte, consolidaremos la canalizaci贸n instanciando formalmente el objeto `Pipeline`. La magia de las canalizaciones reside en su interfaz simplificada: una vez configurada, puedes tratar a toda la secuencia de transformaci贸n y modelado como si fuera un 煤nico estimador. Esto no solo hace que el c贸digo sea m谩s limpio, sino que garantiza que el preprocesamiento sea consistente entre los datos de entrenamiento y los de prueba.


```python
# Importar Pipeline y confusion_matrix (asumiendo que imp_mean y knn est谩n instanciados)
from sklearn.pipeline import Pipeline
from sklearn.metrics import confusion_matrix

# Definici贸n de pasos del ejercicio anterior
steps = [("imputer", imp_mean),
        ("knn", knn)]

# 1. Crear la canalizaci贸n (Pipeline)
pipeline = Pipeline(steps)

# 2. Ajustar la canalizaci贸n a los datos de entrenamiento
# El pipeline ejecuta autom谩ticamente imputer.fit_transform() y luego knn.fit()
pipeline.fit(X_train, y_train)

# 3. Hacer predicciones sobre el conjunto de pruebas
# El pipeline aplica imputer.transform() antes de llamar a knn.predict()
y_pred = pipeline.predict(X_test)

# 4. Imprimir la matriz de confusi贸n para evaluar el rendimiento
print(confusion_matrix(y_test, y_pred))
```

---

### An谩lisis del Flujo de Trabajo con Pipeline

El uso de `Pipeline` es una de las mejores pr谩cticas en el aprendizaje autom谩tico moderno por varias razones t茅cnicas fundamentales:

- **Interfaz Unificada:** Al encapsular el `SimpleImputer` y el `KNeighborsClassifier`, solo necesitas llamar a `.fit()` y `.predict()` una vez. El `Pipeline` se encarga de coordinar qu茅 datos pasan por cada etapa y si debe aplicar un `fit_transform` (en entrenamiento) o solo un `transform` (en prueba).
    
- **Prevenci贸n de Errores Manuales:** Es com煤n olvidar aplicar una transformaci贸n a los datos de prueba o, peor a煤n, aplicar un `fit` sobre los datos de prueba por error. El `Pipeline` elimina este riesgo al automatizar la l贸gica interna.
    
- **Evaluaci贸n con Matriz de Confusi贸n:** Al final del flujo, la `confusion_matrix` nos permite ver exactamente c贸mo est谩n interactuando la imputaci贸n y la clasificaci贸n. Si el modelo tiene muchos falsos negativos, podr铆amos investigar si la estrategia de imputaci贸n (la media) est谩 desdibujando las diferencias entre los g茅neros musicales.
    

>  **Nota de Ingenier铆a:** En entornos de producci贸n, este objeto `pipeline` es lo que se "serializa" (se guarda como un archivo `.pkl` o `.joblib`). Cuando el sistema recibe una nueva canci贸n para clasificar, el archivo cargado se encarga de rellenar los valores faltantes y predecir el g茅nero en un solo paso, asegurando que el modelo se comporte exactamente igual que durante su desarrollo.
