Utilizaremos el conjunto de datos `diabetes_df` para construir un modelo de **Regresi√≥n Log√≠stica**. A diferencia de los modelos previos que devuelven una clase directa (0 o 1), aqu√≠ nos enfocaremos en obtener la **probabilidad** de pertenencia a la clase positiva. Esto es fundamental en contextos m√©dicos, donde conocer la certeza del diagn√≥stico es tan importante como el diagn√≥stico mismo. El conjunto de datos ya ha sido dividido en `X_train`, `X_test`, `y_train` e `y_test`.


```python
# Importar LogisticRegression
from sklearn.linear_model import LogisticRegression

# Instanciar el modelo de regresi√≥n log√≠stica
logreg = LogisticRegression()

# Ajustar el modelo a los datos de entrenamiento
logreg.fit(X_train, y_train)

# Predecir probabilidades para el conjunto de prueba
# Seleccionamos la segunda columna [:, 1] que corresponde a la clase positiva (diabetes)
y_pred_probs = logreg.predict_proba(X_test)[:, 1]

# Visualizar las primeras diez probabilidades estimadas
print(y_pred_probs[:10])
```

---

### An√°lisis de la Predicci√≥n de Probabilidades

En este paso, hemos pasado de una clasificaci√≥n "r√≠gida" a una "probabil√≠stica". Aqu√≠ te explico los detalles t√©cnicos clave para tu formaci√≥n como ingeniero:

- **`predict_proba` vs `predict`**: Mientras que `.predict()` te dar√≠a un 0 o un 1 bas√°ndose en el umbral por defecto (0.5), `.predict_proba()` te devuelve una matriz con dos columnas. La primera es la probabilidad de ser clase 0 y la segunda la de ser clase 1.
    
- **Slicing `[:, 1]`**: En Python, esta nomenclatura es vital. Le indicamos al modelo: "toma todas las filas (`:`) pero qu√©date √∫nicamente con la segunda columna (`1`)". Esa columna representa la probabilidad de que el paciente **s√≠ tenga diabetes**.
    
- **Utilidad en Producci√≥n**: Como aspirante a **DevOps**, piensa en esto como un sistema de _logging_ de criticidad. No es lo mismo una alerta con un 51% de probabilidad que una con un 99%. Obtener las probabilidades te permite crear sistemas de triaje donde los casos con mayor probabilidad se atiendan con mayor prioridad.
    

Al observar la salida de `print(y_pred_probs[:10])`, ver√°s valores entre 0 y 1 (ej. `0.12`, `0.85`). Un `0.85` significa que el modelo tiene un 85% de confianza en que esa observaci√≥n pertenece a la clase positiva.

---
Una vez construido el modelo de regresi√≥n log√≠stica para predecir el estado diab√©tico, el siguiente paso cr√≠tico es evaluar su desempe√±o en todos los umbrales de decisi√≥n posibles. La **Curva ROC** permite visualizar la relaci√≥n entre la tasa de verdaderos positivos (sensibilidad) y la tasa de falsos positivos (1 - especificidad). El objetivo es observar qu√© tan lejos se encuentra nuestra curva de la l√≠nea de azar (la l√≠nea punteada), lo cual indica la capacidad discriminatoria del modelo.

```python
# Importar roc_curve desde sklearn.metrics
from sklearn.metrics import roc_curve

# Generar los valores de la curva ROC: fpr, tpr, thresholds
# Pasamos las etiquetas reales y las probabilidades de la clase positiva
fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)

# Dibujar la l√≠nea punteada que representa un modelo aleatorio (base)
plt.plot([0, 1], [0, 1], 'k--')

# Graficar la tasa de verdaderos positivos (tpr) frente a la de falsos positivos (fpr)
plt.plot(fpr, tpr)

# Configuraci√≥n de etiquetas y t√≠tulo
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Diabetes Prediction')

# Mostrar el gr√°fico
plt.show()
```

---

### An√°lisis de la Curva ROC

La Curva ROC es una de las herramientas de diagn√≥stico m√°s potentes en el aprendizaje supervisado. Aqu√≠ te detallo los puntos clave para interpretarla como un experto:

#### 1. Los Ejes: TPR y FPR

- **True Positive Rate (TPR):** Tambi√©n conocido como _Recall_ o Sensibilidad. Representa la proporci√≥n de individuos con diabetes que el modelo identific√≥ correctamente.
    
- **False Positive Rate (FPR):** Es la proporci√≥n de individuos sanos que el modelo clasific√≥ err√≥neamente como diab√©ticos. Queremos que este valor sea lo m√°s bajo posible.
    

#### 2. La L√≠nea Punteada ($y = x$)

Esta l√≠nea representa un modelo que no tiene capacidad predictiva, es decir, un modelo que adivina al azar (como lanzar una moneda). Cualquier modelo √∫til debe tener una curva que se sit√∫e **por encima** de esta l√≠nea.

#### 3. El "Codo" de la Curva

El objetivo ideal es que la curva se acerque lo m√°s posible a la **esquina superior izquierda** del gr√°fico. Ese punto representa un escenario con un TPR de 1.0 (detectamos a todos los enfermos) y un FPR de 0.0 (no damos falsas alarmas).

- **Punto de equilibrio:** Al observar la curva, puedes elegir un umbral espec√≠fico que equilibre el costo de un falso positivo frente al riesgo de un falso negativo. En medicina, a menudo aceptamos un FPR ligeramente m√°s alto si eso nos garantiza un TPR (detecci√≥n) cercano al 100%.
    

#### 4. Relaci√≥n con el AUC

Aunque este ejercicio se centra en la visualizaci√≥n, recuerda que el √°rea total debajo de esta l√≠nea azul es el **AUC**. Cuanto m√°s "abombada" est√© la curva hacia arriba, mayor ser√° el √°rea y, por lo tanto, mejor ser√° el modelo para distinguir entre las dos clases.

---
Tras haber visualizado la curva ROC, el paso final es cuantificar esa capacidad de discriminaci√≥n mediante la m√©trica **AUC** (_Area Under the Curve_). En este ejercicio, calcularemos el √°rea bajo la curva para el modelo de regresi√≥n log√≠stica y lo compararemos con la matriz de confusi√≥n y el reporte de clasificaci√≥n. Esto nos permitir√° tener una visi√≥n de $360^{\circ}$ sobre qu√© tan bien el modelo separa a los individuos sanos de los diab√©ticos en comparaci√≥n con otros modelos como KNN.


```python
# Importar roc_auc_score desde sklearn.metrics
from sklearn.metrics import roc_auc_score

# 1. Calcular e imprimir la puntuaci√≥n ROC AUC
# Pasamos las etiquetas reales y las probabilidades de la clase positiva
print(roc_auc_score(y_test, y_pred_probs))

# 2. Calcular e imprimir la matriz de confusi√≥n
# Comparamos etiquetas reales con las etiquetas predichas (0 o 1)
print(confusion_matrix(y_test, y_pred))

# 3. Calcular e imprimir el informe de clasificaci√≥n
print(classification_report(y_test, y_pred))
```

---

### An√°lisis Integral de M√©tricas

Para un ingeniero de datos o un especialista en modelos predictivos, el AUC es la "prueba de fuego" de la robustez de un clasificador binario.

#### 1. ¬øQu√© nos dice el ROC AUC Score?

El valor del AUC representa la probabilidad de que el modelo asigne una probabilidad m√°s alta a un caso positivo elegido al azar que a un caso negativo elegido al azar.

- **$AUC = 0.5$**: El modelo no tiene capacidad de discriminaci√≥n (es como lanzar una moneda).
    
- **$0.7 \leq AUC < 0.8$**: Se considera una capacidad aceptable.
    
- **$0.8 \leq AUC < 0.9$**: Se considera una capacidad excelente.
    
- **$AUC \geq 0.9$**: Es una capacidad excepcional.
    

#### 2. La Triangulaci√≥n: AUC, Matriz e Informe

- **El AUC** te da una m√©trica global de la capacidad del modelo para "ordenar" las observaciones por probabilidad, independientemente del umbral.
    
- **La Matriz de Confusi√≥n** te muestra exactamente d√≥nde est√° fallando el modelo (¬øestamos dejando ir a muchos diab√©ticos o estamos asustando a gente sana?).
    
- **El Informe de Clasificaci√≥n** te da el detalle de **Precision** (calidad) y **Recall** (cantidad) para cada clase.
    

#### 3. Comparativa de Modelos

En este ejercicio, al comparar los resultados de la **Regresi√≥n Log√≠stica** contra los de **KNN**, notar√°s que uno suele tener un AUC m√°s alto. Generalmente, la Regresi√≥n Log√≠stica es muy robusta para problemas m√©dicos porque su naturaleza probabil√≠stica se adapta mejor a la variabilidad de los datos biol√≥gicos que la votaci√≥n por cercan√≠a de KNN.

> üí° **Nota T√©cnica:** El AUC es especialmente valioso cuando tienes un **desequilibrio de clases**, ya que no se ve tan afectado por el tama√±o de las clases como la m√©trica de _Accuracy_.
