## Pipelines en Spark ML: Agilizando el Flujo de Trabajo 

Construir un modelo de Machine Learning rara vez es un solo paso. Implica una secuencia de tareas de preparaci贸n de datos, ingenier铆a de caracter铆sticas y, finalmente, el entrenamiento del modelo.

Un **Pipeline** (tuber铆a o canalizaci贸n) es un mecanismo que nos permite **combinar y encadenar** todos estos pasos en una sola unidad de trabajo.

-----

### El Problema: Fuga de Informaci贸n (Data Leakage) 

Uno de los errores m谩s graves y comunes en ML es la **fuga de informaci贸n**. Ocurre cuando informaci贸n del conjunto de prueba (que deber铆a ser "invisible" para el modelo durante el entrenamiento) se filtra accidentalmente en el proceso de entrenamiento.

  * **Causa Com煤n:** Aplicar transformaciones que "aprenden" de los datos (como `StringIndexer` o la normalizaci贸n) a **todo el conjunto de datos antes de dividirlo**, o aplicar el m茅todo `.fit()` incorrectamente a los datos de prueba.
  * **Consecuencia:** El modelo parece tener un rendimiento espectacular durante el desarrollo, pero falla estrepitosamente en producci贸n porque "hizo trampa" durante la evaluaci贸n.
  * **Regla de Oro:** Para tener resultados s贸lidos, el m茅todo `.fit()` (que aprende par谩metros de los datos) debe aplicarse **NICAMENTE a los datos de entrenamiento**. El m茅todo `.transform()` se aplica tanto a entrenamiento como a prueba.

-----

### La Soluci贸n: Pipelines

Los Pipelines solucionan este problema y simplifican enormemente el c贸digo al encapsular todo el flujo.

  * **Concepto:** En lugar de aplicar cada paso (indexar, codificar, ensamblar, entrenar) individualmente y gestionar manualmente qu茅 datos van a d贸nde, agrupamos todos los pasos en un objeto `Pipeline` y lo ejecutamos como una sola unidad.
  * **Componentes de un Pipeline:**
      * **Transformadores (`Transformer`):** Algoritmos que transforman un DataFrame en otro (ej. `OneHotEncoder`, `VectorAssembler`, o un modelo ya entrenado). Tienen un m茅todo `.transform()`.
      * **Estimadores (`Estimator`):** Algoritmos que se ajustan a los datos para producir un Transformador (ej. `StringIndexer`, `LinearRegression`). Tienen un m茅todo `.fit()`.

-----

### Implementaci贸n en Spark

Un Pipeline se define como una secuencia de etapas (`stages`).

1.  **Definir las etapas individuales:**

    ```python
    from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler
    from pyspark.ml.regression import LinearRegression

    # Etapa 1: Indexar la columna 'type'
    indexer = StringIndexer(inputCol='type', outputCol='type_idx')

    # Etapa 2: One-hot encoding
    onehot = OneHotEncoder(inputCols=['type_idx'], outputCols=['type_dummy'])

    # Etapa 3: Ensamblar todas las caracter铆sticas en un vector
    assemble = VectorAssembler(inputCols=['mass', 'cyl', 'type_dummy'], outputCol='features')

    # Etapa 4: El modelo de regresi贸n
    regression = LinearRegression(labelCol='consumption')
    ```

2.  **Crear el Pipeline:**
    Unimos las etapas en el orden correcto.

    ```python
    from pyspark.ml import Pipeline

    # Crear el pipeline con la lista de etapas ordenadas
    pipeline = Pipeline(stages=[indexer, onehot, assemble, regression])
    ```

3.  **Entrenar y Usar el Pipeline:**

      * Al llamar a `pipeline.fit(training_data)`, Spark ejecuta autom谩ticamente `.fit()` y `.transform()` en secuencia para todas las etapas usando **solo los datos de entrenamiento**. Esto garantiza que no haya fugas.
      * El resultado es un `PipelineModel` que sabe c贸mo realizar todos los pasos de transformaci贸n y predicci贸n.
      * Para hacer predicciones, simplemente llamamos a `pipeline_model.transform(test_data)`.

    <!-- end list -->

    ```python
    # Entrenar todo el flujo de una sola vez
    pipeline_model = pipeline.fit(cars_train)

    # Hacer predicciones en datos nuevos autom谩ticamente
    predictions = pipeline_model.transform(cars_test)
    ```

-----

### Acceso a las Etapas Internas

Si necesitas inspeccionar una parte espec铆fica del modelo entrenado (por ejemplo, para ver los coeficientes de la regresi贸n lineal), puedes acceder a las etapas individuales del `PipelineModel` usando su 铆ndice.

```python
# Acceder a la etapa 3 (que es LinearRegression, 铆ndice 3 porque empezamos en 0)
regression_model = pipeline_model.stages[3]

# Ver el intercepto y los coeficientes
print(regression_model.intercept)
print(regression_model.coefficients)
```
