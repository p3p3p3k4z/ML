## Grid Search: Ajuste de Hiperpar치metros en Spark ML 游꿢

El **ajuste de hiperpar치metros** es el proceso de encontrar la mejor configuraci칩n para un modelo de Machine Learning antes de comenzar el entrenamiento. El rendimiento del modelo puede mejorar dr치sticamente con los par치metros correctos.

-----

### El Problema: 쯈u칠 par치metros elegir?

Cada algoritmo de ML tiene "perillas" que podemos ajustar. Por ejemplo, en una regresi칩n lineal regularizada, tenemos:

  * `fitIntercept`: 쮻eber칤a el modelo calcular una intersecci칩n (ordenada al origen)?
  * `regParam` ($\lambda$): 쯈u칠 tan fuerte debe ser la regularizaci칩n?
  * `elasticNetParam` ($\alpha$): 쯈u칠 tipo de regularizaci칩n usar (Ridge, Lasso, ElasticNet)?

Probar manualmente cada combinaci칩n es tedioso y propenso a errores. Adem치s, usar una **칰nica estimaci칩n del RMSE** (divisi칩n simple train/test) para comparar estas combinaciones no es muy s칩lido, ya que podr칤a depender de la suerte de esa divisi칩n espec칤fica.

-----

### La Soluci칩n: Grid Search con Cross-Validation 游댍

La **B칰squeda en Cuadr칤cula (Grid Search)** es una t칠cnica sistem치tica para probar m칰ltiples combinaciones de par치metros.

  * **Concepto:** Definimos una "cuadr칤cula" de valores posibles para cada par치metro. El algoritmo prueba *todas* las combinaciones posibles de estos valores.
  * **Evaluaci칩n Robusta:** Para cada combinaci칩n, se construye y eval칰a un modelo utilizando **Validaci칩n Cruzada (Cross-Validation)**. Esto nos da una estimaci칩n mucho m치s confiable de qu칠 tan bien funcionar치 esa configuraci칩n en la realidad.
  * **Selecci칩n:** Al final, elegimos la combinaci칩n de par치metros que tuvo el mejor rendimiento promedio en la validaci칩n cruzada.

-----

### Implementaci칩n en Spark ML

En Spark, usamos `ParamGridBuilder` para definir la cuadr칤cula y `CrossValidator` para ejecutar la b칰squeda.

#### 1\. Construir la Cuadr칤cula de Par치metros (`ParamGridBuilder`)

Definimos qu칠 par치metros y qu칠 valores queremos probar.

```python
from pyspark.ml.tuning import ParamGridBuilder

# Supongamos que tenemos un modelo 'regression' (LinearRegression)
# Queremos probar:
# - fitIntercept: True o False (2 valores)
# - regParam: 0.001, 0.01, 0.1, 1.0, 10.0 (5 valores)
# - elasticNetParam: 0.0, 0.25, 0.5, 0.75, 1.0 (5 valores)

params = ParamGridBuilder() \
    .addGrid(regression.fitIntercept, [True, False]) \
    .addGrid(regression.regParam, [0.001, 0.01, 0.1, 1.0, 10.0]) \
    .addGrid(regression.elasticNetParam, [0.0, 0.25, 0.5, 0.75, 1.0]) \
    .build()

# Total de modelos a probar: 2 * 5 * 5 = 50 combinaciones.
print('N칰mero de modelos a probar:', len(params))
```

#### 2\. Ejecutar la B칰squeda con `CrossValidator`

Configuramos el `CrossValidator` con el estimador (modelo o pipeline), la cuadr칤cula (`params`) y el evaluador.

```python
from pyspark.ml.tuning import CrossValidator
from pyspark.ml.evaluation import RegressionEvaluator

# Crear el evaluador (RMSE)
evaluator = RegressionEvaluator(labelCol='consumption', metricName='rmse')

# Crear el CrossValidator
# numFolds=10 significa que para CADA una de las 50 combinaciones,
# se entrenar치n 10 modelos (uno por fold). Total = 500 entrenamientos.
cv = CrossValidator(estimator=regression,
                    estimatorParamMaps=params,
                    evaluator=evaluator,
                    numFolds=10,
                    seed=13)

# Ejecutar la b칰squeda (춰esto puede tardar!)
cv_model = cv.fit(cars_train)
```

#### 3\. El Mejor Modelo y sus Par치metros

Una vez que termina `.fit()`, el objeto resultante `cv_model` es el **mejor modelo** encontrado, ya re-entrenado con todos los datos de entrenamiento usando los mejores par치metros.

  * **Usar el mejor modelo:**

    ```python
    # Hacer predicciones directamente con el mejor modelo
    predictions = cv_model.transform(cars_test)
    ```

  * **Inspeccionar los mejores par치metros:**
    Podemos acceder al mejor modelo subyacente y ver qu칠 par치metros ganaron.

    ```python
    # Acceder al mejor modelo
    best_model = cv_model.bestModel

    # Ver un par치metro espec칤fico (ej. fitIntercept)
    print(best_model.getOrDefault('fitIntercept'))

    # O ver todos los par치metros explicados
    # print(best_model.explainParams())
    ```

El Grid Search con Cross-Validation es el est치ndar de oro para ajustar modelos, asegurando que hemos explorado sistem치ticamente las opciones y elegido la configuraci칩n m치s robusta.