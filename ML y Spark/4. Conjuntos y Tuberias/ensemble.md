## M√©todos de Ensamble (Ensemble Methods) en Spark ML üå≥üå≤üå≥

Los m√©todos de ensamble son una t√©cnica avanzada y poderosa en Machine Learning que se basa en una idea simple pero efectiva: **la uni√≥n hace la fuerza**.

-----

### Concepto Clave: El "Comit√©" de Expertos

  * **Definici√≥n:** Un ensamble consiste en crear una **colecci√≥n de modelos** (a menudo llamados "estimadores base" o "aprendices d√©biles") y **combinar sus resultados** para obtener una predicci√≥n final √∫nica.
  * **Analog√≠a:** Piensa en un **comit√©**. La decisi√≥n tomada por un grupo de expertos diversos suele ser mejor y m√°s robusta que la decisi√≥n de un solo individuo, por muy inteligente que sea.
  * **Sabidur√≠a de la multitud (Wisdom of the Crowd):** Si los modelos individuales son ligeramente mejores que el azar y son lo suficientemente diversos e independientes, su combinaci√≥n puede producir predicciones mucho m√°s precisas.
  * **Diversidad:** Para que el comit√© funcione, sus miembros deben pensar diferente. En ML, **entre m√°s diversos sean los modelos, mejor** ser√° el ensamble. Si todos cometen los mismos errores, el ensamble no mejora nada.

Existen dos familias principales de m√©todos de ensamble: **Bagging** (ej. Random Forest) y **Boosting** (ej. Gradient-Boosted Trees).

-----

### 1\. Random Forest (Bosque Aleatorio) - Bagging üéí

Random Forest es el ejemplo cl√°sico de la t√©cnica **Bagging** (Bootstrap Aggregating). Su objetivo es reducir la varianza (hacer el modelo m√°s estable y menos propenso al sobreajuste).

  * **¬øC√≥mo funciona?**

    1.  Crea muchos √°rboles de decisi√≥n independientes.
    2.  Cada √°rbol se entrena con un **subconjunto aleatorio diferente de los datos** (muestreo con reemplazo o *bootstrapping*).
    3.  Adem√°s, en cada nodo de cada √°rbol, se considera solo un **subconjunto aleatorio de caracter√≠sticas** para hacer la divisi√≥n. Esto garantiza a√∫n m√°s **diversidad** e independencia entre los √°rboles.
    4.  **Paralelismo:** Como los √°rboles son independientes, **se pueden entrenar en paralelo**, lo que lo hace muy r√°pido y escalable.
    5.  **Predicci√≥n Final:**
          * *Clasificaci√≥n:* Votaci√≥n por mayor√≠a (la clase que m√°s √°rboles predijeron).
          * *Regresi√≥n:* Promedio de las predicciones de todos los √°rboles.

  * **En Spark ML:**

    ```python
    from pyspark.ml.classification import RandomForestClassifier

    # Crear el bosque
    # numTrees: cu√°ntos √°rboles (miembros del comit√©) queremos.
    forest = RandomForestClassifier(numTrees=5)

    # Entrenar (los 5 √°rboles se entrenan en paralelo si es posible)
    forest_model = forest.fit(cars_train)
    ```

  * **Inspecci√≥n:** Podemos incluso ver los √°rboles individuales dentro del bosque.

    ```python
    # Ver los √°rboles individuales
    print(forest_model.trees)
    ```

  * **Importancia de Caracter√≠sticas:** Random Forest nos dice qu√© caracter√≠sticas fueron m√°s √∫tiles para tomar decisiones.

    ```python
    # Ver la importancia de cada caracter√≠stica
    print(forest_model.featureImportances)
    ```

-----

### 2\. Gradient-Boosted Trees (GBT) - Boosting üöÄ

Gradient-Boosted Trees utiliza la t√©cnica de **Boosting**. Aqu√≠, los modelos no son independientes; trabajan en equipo secuencialmente para reducir el sesgo (mejorar la precisi√≥n).

  * **¬øC√≥mo funciona?**

    1.  Entrena una **secuencia de modelos** (√°rboles), uno tras otro.
    2.  Cada modelo nuevo **intenta corregir los errores** cometidos por la combinaci√≥n de los modelos anteriores. Se enfoca en los casos "dif√≠ciles" que los otros no pudieron resolver bien.
    3.  **Secuencial:** NO se pueden entrenar en paralelo, porque el √°rbol 2 necesita saber qu√© tan mal lo hizo el √°rbol 1. Esto puede hacerlo m√°s lento de entrenar que Random Forest.
    4.  **Mejora Iterativa:** El modelo mejora con cada iteraci√≥n, pero tambi√©n tiene mayor riesgo de sobreajuste si se usan demasiadas.

  * **En Spark ML:**

    ```python
    from pyspark.ml.classification import GBTClassifier

    # Crear el modelo GBT
    gbt = GBTClassifier(maxIter=10) # 10 √°rboles secuenciales

    # Entrenar (debe ser secuencial)
    gbt_model = gbt.fit(train_data)
    ```

-----

### Comparaci√≥n de Rendimiento

Ambos m√©todos suelen superar a un simple √°rbol de decisi√≥n.

  * **√Årbol de Decisi√≥n:** Simple, interpretable, pero propenso a sobreajuste.
  * **Random Forest:** Robusto, paralelizable, bueno "casi siempre" sin mucho ajuste.
  * **GBT:** A menudo puede lograr una precisi√≥n ligeramente mayor que Random Forest si se ajusta bien, pero es m√°s lento de entrenar y m√°s sensible a los hiperpar√°metros.

*(En la imagen de ejemplo, tanto RF como GBT superan al √°rbol simple, con un AUC de 0.65 vs 0.58)*.