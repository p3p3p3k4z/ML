## Validaci√≥n Cruzada (Cross-Validation) en Spark ML

La validaci√≥n cruzada es una t√©cnica esencial para evaluar la robustez de un modelo de ML y para ajustar sus hiperpar√°metros de manera confiable.

-----

### El Problema: Limitaciones de una sola divisi√≥n

El enfoque m√°s simple es dividir los datos aleatoriamente en un conjunto de entrenamiento y uno de prueba.

  * **Inconveniente:** Solo obtienes una √∫nica estimaci√≥n del rendimiento. Si por casualidad el conjunto de prueba fue "f√°cil" o "dif√≠cil", tu estimaci√≥n podr√≠a ser enga√±osa.
  * **Idea:** ¬øQu√© pasar√≠a si pudi√©ramos probar el modelo varias veces con diferentes divisiones de datos para tener una idea m√°s s√≥lida de su rendimiento real?

-----

### La Soluci√≥n: K-Fold Cross-Validation üîÑ

La validaci√≥n cruzada de K pliegues (*K-Fold Cross-Validation*) es el m√©todo est√°ndar.

1.  **Divisi√≥n Inicial:** Comienza con el conjunto de datos de entrenamiento completo (es importante aleatorizarlo primero).
2.  **Particiones (Folds):** Divide estos datos en $K$ particiones (o *folds*) de igual tama√±o. El n√∫mero $K$ influye en el nombre (ej. 10-fold CV).
3.  **Proceso Iterativo:**
      * En la iteraci√≥n 1: Usa el **Fold 1 como validaci√≥n** y los Folds 2 a $K$ para entrenar. Eval√∫a y guarda la m√©trica.
      * En la iteraci√≥n 2: Usa el **Fold 2 como validaci√≥n** y el resto para entrenar. Eval√∫a y guarda la m√©trica.
      * ... Repite $K$ veces.
4.  **Resultado Final:** El rendimiento del modelo es el **promedio** de las m√©tricas obtenidas en las $K$ iteraciones.

-----

### Ajuste de Hiperpar√°metros (Grid Search)

La validaci√≥n cruzada brilla realmente cuando queremos encontrar la mejor combinaci√≥n de hiperpar√°metros (ej. los valores de `regParam` y `elasticNetParam` en una regresi√≥n).

  * **Grid Search:** Definimos una "cuadr√≠cula" de posibles valores para cada par√°metro.
  * **Proceso:** Para *cada* combinaci√≥n de par√°metros en la cuadr√≠cula, ejecutamos todo el proceso de validaci√≥n cruzada.
  * **Selecci√≥n:** El modelo final es el que tuvo el mejor rendimiento promedio en la validaci√≥n cruzada.

-----

### Implementaci√≥n en Spark ML

En Spark, usamos `CrossValidator` junto con `ParamGridBuilder`.

1.  **Definir la Cuadr√≠cula de Par√°metros:**

    ```python
    from pyspark.ml.tuning import ParamGridBuilder, CrossValidator

    # Construir la cuadr√≠cula de hiperpar√°metros a probar
    # Ej: probaremos regParam=0.01, 0.1, 1.0 y elasticNetParam=0.0, 0.5, 1.0
    # Esto resulta en 3 x 3 = 9 combinaciones a probar.
    params = ParamGridBuilder() \
        .addGrid(regression.regParam, [0.01, 0.1, 1.0]) \
        .addGrid(regression.elasticNetParam, [0.0, 0.5, 1.0]) \
        .build()
    ```

2.  **Configurar el CrossValidator:**
    Necesita saber qu√© estimador usar (tu modelo o pipeline), qu√© par√°metros probar (la cuadr√≠cula) y c√≥mo evaluar cada intento (un evaluador).

    ```python
    # Crear el validador cruzado
    # numFolds=10 es un valor com√∫n (pero costoso computacionalmente).
    cv = CrossValidator(estimator=regression, # O tu 'pipeline' completo
                        estimatorParamMaps=params,
                        evaluator=evaluator, # Ej: RegressionEvaluator con RMSE
                        numFolds=10,
                        seed=13) # Semilla para reproducibilidad en la divisi√≥n de folds
    ```

3.  **Entrenar (y esperar...):**
    El `CrossValidator` act√∫a como un estimador. Al llamar a `.fit()`, ejecuta todas las combinaciones y folds. ¬°Esto puede tardar mucho\!

    ```python
    # Entrena los 9 modelos x 10 folds = 90 entrenamientos en total.
    cv_model = cv.fit(cars_train)
    ```

4.  **Mejor Modelo:**
    El objeto `cv_model` resultante se comporta como el mejor modelo encontrado. Puedes usarlo directamente para hacer predicciones.

    ```python
    # Usar el mejor modelo para predecir en el conjunto de prueba REAL (final)
    predictions = cv_model.transform(cars_test)

    # Ver el rendimiento promedio del mejor modelo durante la validaci√≥n cruzada
    print(max(cv_model.avgMetrics)) # O min() si la m√©trica es de error como RMSE
    ```