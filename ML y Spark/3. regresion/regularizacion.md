## Regularizaci√≥n: Evitando el Sobreajuste üõ°Ô∏è

La **regularizaci√≥n** es una t√©cnica fundamental en Machine Learning para crear modelos m√°s robustos y generalizables, especialmente cuando se trabaja con muchas caracter√≠sticas. Su objetivo es evitar que el modelo se vuelva demasiado complejo y se "memorice" los datos de entrenamiento (sobreajuste).

-----

### El Problema de la Complejidad: ¬øPor qu√© Regularizar?

En la regresi√≥n lineal, el modelo intenta encontrar un **coeficiente** (peso) √≥ptimo para cada caracter√≠stica (columna) de tus datos. Estos coeficientes cuantifican el efecto de cada caracter√≠stica en la predicci√≥n.

  * **M√°s caracter√≠sticas = M√°s coeficientes.**
  * **Escenario Ideal:** Tienes muchas filas (datos) y pocas columnas (caracter√≠sticas). Aqu√≠, la regresi√≥n est√°ndar funciona bien.
  * **Escenario Problem√°tico:** Tienes **muchas columnas** y pocas filas. El modelo tiene demasiada libertad y puede encontrar relaciones espurias (ruido) que solo existen en esos datos espec√≠ficos. Esto lleva a un modelo complejo, dif√≠cil de interpretar y que falla con datos nuevos.

**El Objetivo:** Buscamos un modelo **parsimonioso**, es decir, lo m√°s simple posible pero que a√∫n haga buenas predicciones. Queremos seleccionar autom√°ticamente el subconjunto de caracter√≠sticas que realmente importan.

-----

### La Soluci√≥n: Regresi√≥n Penalizada

La regularizaci√≥n funciona modificando la **funci√≥n de p√©rdida** que el algoritmo intenta minimizar durante el entrenamiento.

  * **Funci√≥n de P√©rdida Est√°ndar (MSE):** Solo se preocupa por minimizar el error de predicci√≥n.
      * $P√©rdida = MSE$
  * **Funci√≥n de P√©rdida Regularizada:** A√±ade un **t√©rmino de penalizaci√≥n** que "castiga" al modelo por tener coeficientes grandes.
      * $P√©rdida = MSE + \lambda \cdot Penalizaci√≥n$

Donde $\lambda$ (lambda, o `regParam` en Spark) controla la fuerza de la penalizaci√≥n:

  * Si $\lambda = 0$, no hay regularizaci√≥n (es regresi√≥n est√°ndar).
  * Si $\lambda$ es muy grande, la penalizaci√≥n domina y fuerza a los coeficientes a ser muy peque√±os (o cero).

-----

### Tipos de Regularizaci√≥n: Ridge y Lasso

Existen dos formas principales de definir este t√©rmino de penalizaci√≥n, que tienen efectos diferentes en los coeficientes. En Spark, se controlan con `elasticNetParam`.

#### 1\. Regresi√≥n Ridge (Norma L2)

  * **Penalizaci√≥n:** Se basa en la suma de los **cuadrados** de los coeficientes ($\sum \beta_i^2$).
  * **Efecto:** Reduce **todos** los coeficientes hacia cero de manera proporcional, pero raramente los hace exactamente cero.
  * **Uso:** Ideal cuando crees que **muchas caracter√≠sticas contribuyen un poco** al resultado final.
  * **En Spark:** `elasticNetParam = 0.0`.

<!-- end list -->

```python
# Ridge Regression en Spark
# elasticNetParam=0.0 indica Ridge. regParam=0.1 es la fuerza de la penalizaci√≥n.
ridge = LinearRegression(labelCol='consumption', elasticNetParam=0.0, regParam=0.1)
ridge_model = ridge.fit(cars_train)

# Examinar coeficientes: Ser√°n m√°s peque√±os que sin regularizaci√≥n, pero no cero.
print(ridge_model.coefficients)
```

#### 2\. Regresi√≥n Lasso (Norma L1)

  * **Penalizaci√≥n:** Se basa en la suma de los **valores absolutos** de los coeficientes ($\sum |\beta_i|$).
  * **Efecto:** Tiene la propiedad √∫nica de poder reducir algunos coeficientes **exactamente a cero**.
  * **Uso:** Funciona como una **selecci√≥n autom√°tica de caracter√≠sticas**. Es ideal cuando crees que solo un **peque√±o subconjunto de caracter√≠sticas** es realmente importante y quieres ignorar el resto (modelo parsimonioso).
  * **En Spark:** `elasticNetParam = 1.0`.

<!-- end list -->

```python
# Lasso Regression en Spark
# elasticNetParam=1.0 indica Lasso.
lasso = LinearRegression(labelCol='consumption', elasticNetParam=1.0, regParam=0.1)
lasso_model = lasso.fit(cars_train)

# Examinar coeficientes: Ver√°s varios valores de 0.0, indicando caracter√≠sticas eliminadas.
print(lasso_model.coefficients)
```

#### 3\. Elastic Net

  * Es un compromiso entre ambas. Combina las penalizaciones L1 y L2.
  * **En Spark:** `elasticNetParam` puede ser cualquier valor entre 0.0 y 1.0 (ej. 0.5 para una mezcla 50/50).

-----

### Resumen de Par√°metros Clave en Spark ML

| Par√°metro | Nombre Matem√°tico | Significado | Valores T√≠picos |
| :--- | :---: | :--- | :--- |
| `regParam` | $\lambda$ (Lambda) | **Fuerza** de la regularizaci√≥n. | $\ge 0$. (Ej. 0.01, 0.1, 1.0). Se debe ajustar con validaci√≥n cruzada. |
| `elasticNetParam` | $\alpha$ (Alpha) | **Tipo** de mezcla de regularizaci√≥n. | 0.0 (Ridge), 1.0 (Lasso), o intermedio (Elastic Net). |