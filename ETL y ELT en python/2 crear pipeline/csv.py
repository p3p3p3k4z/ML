import pandas as pd
import os
from datetime import datetime

# =================================================================
# PIPELINE DE DATOS INTEGRADO (FILE-BASED ETL)
# Prop√≥sito: Procesar registros de ventas desde fuentes heterog√©neas
#            asegurando integridad de tipos y persistencia personalizada.
# =================================================================

# --- CONFIGURACI√ìN DE RUTAS ---
INPUT_FILE = "sales_data.parquet"  # Fuente original (Binario/Columnar)
OUTPUT_FILE = "transformed_sales_report.csv" # Destino (Texto Plano/Intercambio)

def extract(file_path):
    """
    FASE 1: EXTRACCI√ìN
    POR QU√â: Manejar diferentes formatos permite al pipeline ser flexible.
    """
    print(f"üì¶ [EXTRACT] Iniciando ingesta desde: {file_path}")
    
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"El archivo {file_path} no existe en el volumen.")

    # L√≥gica de selecci√≥n de motor seg√∫n extensi√≥n
    if file_path.endswith('.parquet'):
        return pd.read_parquet(file_path, engine="fastparquet")
    else:
        return pd.read_csv(file_path)

def transform(df):
    """
    FASE 2: TRANSFORMACI√ìN Y LIMPIEZA
    POR QU√â: Aqu√≠ reducimos la carga computacional y normalizamos datos.
    """
    print("üõ†Ô∏è [TRANSFORM] Aplicando l√≥gica de negocio y tipado...")

    # 1. Normalizaci√≥n de Tipos (Datetime Casting)
    # Vital para que el sistema entienda el orden cronol√≥gico
    df["Order Date"] = pd.to_datetime(df["Order Date"], format="%m/%d/%y %H:%M")

    # 2. Filtrado L√≥gico (Poda de datos)
    # Solo art√≠culos econ√≥micos (< 25$) y pedidos unitarios (== 1)
    # Esto reduce el tama√±o del dataset en RAM
    query_condition = (df["Price Each"] < 25) & (df["Quantity Ordered"] == 1)
    clean_data = df.loc[query_condition, :].copy()

    # 3. Proyecci√≥n (Selecci√≥n de columnas cr√≠ticas)
    cols = ["Order ID", "Product", "Price Each", "Order Date", "Quantity Ordered"]
    return clean_data.loc[:, cols]

def load(df, target_path):
    """
    FASE 3: CARGA Y PERSISTENCIA PERSONALIZADA
    POR QU√â: Formatear la salida facilita la ingesta en procesos posteriores.
    """
    print(f"üíæ [LOAD] Persistiendo datos en: {target_path}")
    
    # Personalizaci√≥n: Sin cabecera (para logs/append), sin √≠ndice (limpieza)
    # y usando separador de tuber√≠a (|) para evitar conflictos con comas.
    df.to_csv(target_path, header=False, index=False, sep="|")
    
    # Verificaci√≥n de Integridad del Sistema de Archivos
    if os.path.exists(target_path):
        size = os.path.getsize(target_path)
        print(f"‚úÖ √âxito: Archivo escrito ({size} bytes).")
    else:
        raise Exception("Error cr√≠tico: El archivo no fue creado por el SO.")

# =================================================================
# ORQUESTACI√ìN (CONTROL DE FLUJO)
# =================================================================

def run_full_pipeline():
    try:
        start_time = datetime.now()
        
        # Ejecuci√≥n secuencial
        data_raw = extract(INPUT_FILE)
        data_clean = transform(data_raw)
        load(data_clean, OUTPUT_FILE)
        
        end_time = datetime.now()
        duration = end_time - start_time
        print(f"üöÄ Pipeline finalizado en {duration.total_seconds():.2f} segundos.")
        
    except Exception as error:
        print(f"üõë [CRITICAL ERROR] Fallo en el pipeline: {error}")

if __name__ == "__main__":
    run_full_pipeline()
